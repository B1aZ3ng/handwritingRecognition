{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 20:44:19.342186: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-11 20:44:19.485659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741697059.539501    4798 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741697059.556320    4798 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 20:44:19.688244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import Augmentor\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"/home/b1az3/code/ml/trainingData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images\n",
    "#--- words.txt ---------------------------------------------------------------#\n",
    "#\n",
    "# iam database word information\n",
    "#\n",
    "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
    "#\n",
    "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
    "#     ok              -> result of word segmentation\n",
    "#                            ok: word was correctly\n",
    "#                            er: segmentation of word can be bad\n",
    "#\n",
    "#     154             -> graylevel to binarize the line containing this word\n",
    "#     1               -> number of components for this word NOTE: dont think this exists idk why\n",
    "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
    "#     AT              -> the grammatical tag for this word, see the\n",
    "#                        file tagset.txt for an explanation\n",
    "#     A               -> the transcription for this word\n",
    "#\n",
    "class img:\n",
    "    def __init__ (this,letter,p1,p2):\n",
    "        this.letter = letter\n",
    "        this.inPath = p1\n",
    "        this.savePath = p2\n",
    "    \n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import directory for dataset 1 - IAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add a pixel of border around each picture, because zooming out uses the edge pixels\n",
    "def addBorder (path):\n",
    "    t = cv2.imread(dataPath+path+\".png\")\n",
    "    pic = np.array(t)\n",
    "    x,y,z = pic.shape #ALSO change colour here if needed\n",
    "    \n",
    "    column = np.array([[[255,255,255]] for i in range (x)]) # to add 1 pixel of white around the picture, because the picture is cropped to exactly where the side contains black and zoom function does not take that well\n",
    "    row = np.array([[[255,255,255] for i in range (y+2)]])\n",
    "    pic = np.concatenate([column,pic,column], axis=1)\n",
    "    pic = np.concatenate([row,pic,row],axis=0)\n",
    "    cv2.imwrite(dataPath+path+\".png\",pic)\n",
    "    return pic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18079\n",
      "1960591\n"
     ]
    }
   ],
   "source": [
    "#import directory for dataset 1 - EMNIST\n",
    "images = []\n",
    "directory = open(dataPath + \"/ascii/words.txt\",\"r\").read().splitlines()\n",
    "len(directory)\n",
    "directory = directory[18:] #first 18 lines are instructions\n",
    "for i in directory:\n",
    "    data = i.split(\" \")\n",
    "    a = data[0].split(\"-\")\n",
    "    loc = \"/\"+a[0]+\"/\"+a[0]+\"-\"+a[1]+\"/\"+data[0]\n",
    "    letter = data[-1]\n",
    "    if len(letter) == 1:\n",
    "        images.append(img(letter,\"/words\"+loc,\"/IAM\"+loc))\n",
    "print(len(images))\n",
    "\n",
    "if open(\"log.txt\",\"r\").read() == \"\":\n",
    "    open(\"log.txt\",\"w\").write(\"added border\")\n",
    "    for img_ in images:\n",
    "        try:\n",
    "            addBorder(img_.inPath)\n",
    "        except:\n",
    "            print(dataPath+img_.inPath+\".png\")\n",
    "\n",
    "\n",
    "dir = os.listdir(dataPath + \"/by_class/\")\n",
    "for i in dir:\n",
    "    letter = chr(int(\"0x\"+i,0))\n",
    "    subDir = os.listdir(dataPath + \"/by_class/\"+i)\n",
    "    for j in subDir:\n",
    "        if j[-4:] != \".mit\":\n",
    "            imgDir = os.listdir(dataPath + \"/by_class/\"+i+\"/\"+j)\n",
    "            for k in imgDir:\n",
    "                images.append(img(letter,\"/by_class/\"+i+\"/\"+j+\"/\"+k[:-4],\"/EMNIST/\"+i+\"/\"+j+\"/\"+k[:-4]))\n",
    "print (len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/words/a01/a01-000u/a01-000u-00-00 D /IAM/a01/a01-000u/a01-000u-00-00\n",
      "/words/a01/a01-000u/a01-000u-02-05 D /IAM/a01/a01-000u/a01-000u-02-05\n",
      "/words/a01/a01-000u/a01-000u-03-02 D /IAM/a01/a01-000u/a01-000u-03-02\n",
      "/words/a01/a01-000u/a01-000u-04-02 D /IAM/a01/a01-000u/a01-000u-04-02\n",
      "/words/a01/a01-000u/a01-000u-06-01 D /IAM/a01/a01-000u/a01-000u-06-01\n",
      "/words/a01/a01-000u/a01-000u-06-02 D /IAM/a01/a01-000u/a01-000u-06-02\n",
      "/words/a01/a01-000u/a01-000u-06-06 D /IAM/a01/a01-000u/a01-000u-06-06\n",
      "/words/a01/a01-000x/a01-000x-00-00 D /IAM/a01/a01-000x/a01-000x-00-00\n",
      "/words/a01/a01-000x/a01-000x-01-10 D /IAM/a01/a01-000x/a01-000x-01-10\n",
      "/words/a01/a01-000x/a01-000x-02-05 D /IAM/a01/a01-000x/a01-000x-02-05\n",
      "/words/a01/a01-000x/a01-000x-03-04 D /IAM/a01/a01-000x/a01-000x-03-04\n",
      "/words/a01/a01-000x/a01-000x-04-10 D /IAM/a01/a01-000x/a01-000x-04-10\n",
      "/words/a01/a01-000x/a01-000x-05-00 D /IAM/a01/a01-000x/a01-000x-05-00\n",
      "/words/a01/a01-000x/a01-000x-05-04 D /IAM/a01/a01-000x/a01-000x-05-04\n",
      "/words/a01/a01-003/a01-003-00-07 D /IAM/a01/a01-003/a01-003-00-07\n",
      "/words/a01/a01-003/a01-003-00-08 D /IAM/a01/a01-003/a01-003-00-08\n",
      "/words/a01/a01-003/a01-003-02-05 D /IAM/a01/a01-003/a01-003-02-05\n",
      "/words/a01/a01-003/a01-003-05-04 D /IAM/a01/a01-003/a01-003-05-04\n",
      "/words/a01/a01-003/a01-003-06-04 D /IAM/a01/a01-003/a01-003-06-04\n",
      "/words/a01/a01-003/a01-003-09-04 D /IAM/a01/a01-003/a01-003-09-04\n",
      "/words/a01/a01-003/a01-003-09-07 D /IAM/a01/a01-003/a01-003-09-07\n",
      "/words/a01/a01-003/a01-003-10-01 D /IAM/a01/a01-003/a01-003-10-01\n",
      "/words/a01/a01-003u/a01-003u-01-01 D /IAM/a01/a01-003u/a01-003u-01-01\n",
      "/words/a01/a01-003u/a01-003u-01-02 D /IAM/a01/a01-003u/a01-003u-01-02\n",
      "/words/a01/a01-003u/a01-003u-03-02 D /IAM/a01/a01-003u/a01-003u-03-02\n",
      "/words/a01/a01-003u/a01-003u-06-02 D /IAM/a01/a01-003u/a01-003u-06-02\n",
      "/words/a01/a01-003u/a01-003u-07-02 D /IAM/a01/a01-003u/a01-003u-07-02\n",
      "/words/a01/a01-003u/a01-003u-10-04 D /IAM/a01/a01-003u/a01-003u-10-04\n",
      "/words/a01/a01-003u/a01-003u-10-07 D /IAM/a01/a01-003u/a01-003u-10-07\n",
      "/words/a01/a01-003x/a01-003x-00-07 D /IAM/a01/a01-003x/a01-003x-00-07\n",
      "/words/a01/a01-003x/a01-003x-00-08 D /IAM/a01/a01-003x/a01-003x-00-08\n",
      "/words/a01/a01-003x/a01-003x-02-04 D /IAM/a01/a01-003x/a01-003x-02-04\n",
      "/words/a01/a01-003x/a01-003x-04-08 D /IAM/a01/a01-003x/a01-003x-04-08\n",
      "/words/a01/a01-003x/a01-003x-05-06 D /IAM/a01/a01-003x/a01-003x-05-06\n",
      "/words/a01/a01-003x/a01-003x-08-02 D /IAM/a01/a01-003x/a01-003x-08-02\n",
      "/words/a01/a01-003x/a01-003x-08-05 D /IAM/a01/a01-003x/a01-003x-08-05\n",
      "/words/a01/a01-003x/a01-003x-08-09 D /IAM/a01/a01-003x/a01-003x-08-09\n",
      "/words/a01/a01-007/a01-007-00-02 D /IAM/a01/a01-007/a01-007-00-02\n",
      "/words/a01/a01-007/a01-007-01-04 D /IAM/a01/a01-007/a01-007-01-04\n",
      "/words/a01/a01-007/a01-007-02-02 D /IAM/a01/a01-007/a01-007-02-02\n",
      "/words/a01/a01-007/a01-007-03-05 D /IAM/a01/a01-007/a01-007-03-05\n",
      "/words/a01/a01-007/a01-007-05-01 D /IAM/a01/a01-007/a01-007-05-01\n",
      "/words/a01/a01-007/a01-007-08-02 D /IAM/a01/a01-007/a01-007-08-02\n",
      "/words/a01/a01-007/a01-007-08-06 D /IAM/a01/a01-007/a01-007-08-06\n",
      "/words/a01/a01-007u/a01-007u-00-02 D /IAM/a01/a01-007u/a01-007u-00-02\n",
      "/words/a01/a01-007u/a01-007u-01-04 D /IAM/a01/a01-007u/a01-007u-01-04\n",
      "/words/a01/a01-007u/a01-007u-03-05 D /IAM/a01/a01-007u/a01-007u-03-05\n",
      "/words/a01/a01-007u/a01-007u-05-02 D /IAM/a01/a01-007u/a01-007u-05-02\n",
      "/words/a01/a01-007u/a01-007u-08-06 D /IAM/a01/a01-007u/a01-007u-08-06\n",
      "/words/a01/a01-007u/a01-007u-09-03 D /IAM/a01/a01-007u/a01-007u-09-03\n",
      "/words/a01/a01-007x/a01-007x-00-02 D /IAM/a01/a01-007x/a01-007x-00-02\n",
      "/words/a01/a01-007x/a01-007x-00-08 D /IAM/a01/a01-007x/a01-007x-00-08\n",
      "/words/a01/a01-007x/a01-007x-01-04 D /IAM/a01/a01-007x/a01-007x-01-04\n",
      "/words/a01/a01-007x/a01-007x-03-02 D /IAM/a01/a01-007x/a01-007x-03-02\n",
      "/words/a01/a01-007x/a01-007x-04-06 D /IAM/a01/a01-007x/a01-007x-04-06\n",
      "/words/a01/a01-007x/a01-007x-07-05 D /IAM/a01/a01-007x/a01-007x-07-05\n",
      "/words/a01/a01-007x/a01-007x-07-09 D /IAM/a01/a01-007x/a01-007x-07-09\n",
      "/words/a01/a01-011/a01-011-01-01 D /IAM/a01/a01-011/a01-011-01-01\n",
      "/words/a01/a01-011/a01-011-01-04 D /IAM/a01/a01-011/a01-011-01-04\n",
      "/words/a01/a01-011/a01-011-02-02 D /IAM/a01/a01-011/a01-011-02-02\n",
      "/words/a01/a01-011/a01-011-02-04 D /IAM/a01/a01-011/a01-011-02-04\n",
      "/words/a01/a01-011/a01-011-03-02 D /IAM/a01/a01-011/a01-011-03-02\n",
      "/words/a01/a01-011/a01-011-03-07 D /IAM/a01/a01-011/a01-011-03-07\n",
      "/words/a01/a01-011/a01-011-05-03 D /IAM/a01/a01-011/a01-011-05-03\n",
      "/words/a01/a01-011/a01-011-05-08 D /IAM/a01/a01-011/a01-011-05-08\n",
      "/words/a01/a01-011/a01-011-06-03 D /IAM/a01/a01-011/a01-011-06-03\n",
      "/words/a01/a01-011/a01-011-06-07 D /IAM/a01/a01-011/a01-011-06-07\n",
      "/words/a01/a01-011/a01-011-06-11 D /IAM/a01/a01-011/a01-011-06-11\n",
      "/words/a01/a01-011u/a01-011u-02-00 D /IAM/a01/a01-011u/a01-011u-02-00\n",
      "/words/a01/a01-011u/a01-011u-02-03 D /IAM/a01/a01-011u/a01-011u-02-03\n",
      "/words/a01/a01-011u/a01-011u-03-03 D /IAM/a01/a01-011u/a01-011u-03-03\n",
      "/words/a01/a01-011u/a01-011u-03-05 D /IAM/a01/a01-011u/a01-011u-03-05\n",
      "/words/a01/a01-011u/a01-011u-04-06 D /IAM/a01/a01-011u/a01-011u-04-06\n",
      "/words/a01/a01-011u/a01-011u-05-03 D /IAM/a01/a01-011u/a01-011u-05-03\n",
      "/words/a01/a01-011u/a01-011u-07-04 D /IAM/a01/a01-011u/a01-011u-07-04\n",
      "/words/a01/a01-011u/a01-011u-08-02 D /IAM/a01/a01-011u/a01-011u-08-02\n",
      "/words/a01/a01-011u/a01-011u-08-06 D /IAM/a01/a01-011u/a01-011u-08-06\n",
      "/words/a01/a01-011u/a01-011u-09-03 D /IAM/a01/a01-011u/a01-011u-09-03\n",
      "/words/a01/a01-011u/a01-011u-09-07 D /IAM/a01/a01-011u/a01-011u-09-07\n",
      "/words/a01/a01-011x/a01-011x-01-03 D /IAM/a01/a01-011x/a01-011x-01-03\n",
      "/words/a01/a01-011x/a01-011x-01-06 D /IAM/a01/a01-011x/a01-011x-01-06\n",
      "/words/a01/a01-011x/a01-011x-03-01 D /IAM/a01/a01-011x/a01-011x-03-01\n",
      "/words/a01/a01-011x/a01-011x-03-03 D /IAM/a01/a01-011x/a01-011x-03-03\n",
      "/words/a01/a01-011x/a01-011x-04-02 D /IAM/a01/a01-011x/a01-011x-04-02\n",
      "/words/a01/a01-011x/a01-011x-04-07 D /IAM/a01/a01-011x/a01-011x-04-07\n",
      "/words/a01/a01-011x/a01-011x-07-01 D /IAM/a01/a01-011x/a01-011x-07-01\n",
      "/words/a01/a01-011x/a01-011x-07-06 D /IAM/a01/a01-011x/a01-011x-07-06\n",
      "/words/a01/a01-011x/a01-011x-08-01 D /IAM/a01/a01-011x/a01-011x-08-01\n",
      "/words/a01/a01-011x/a01-011x-08-05 D /IAM/a01/a01-011x/a01-011x-08-05\n",
      "/words/a01/a01-011x/a01-011x-08-09 D /IAM/a01/a01-011x/a01-011x-08-09\n",
      "/words/a01/a01-014/a01-014-02-01 D /IAM/a01/a01-014/a01-014-02-01\n",
      "/words/a01/a01-014/a01-014-02-07 D /IAM/a01/a01-014/a01-014-02-07\n",
      "/words/a01/a01-014/a01-014-03-00 D /IAM/a01/a01-014/a01-014-03-00\n",
      "/words/a01/a01-014/a01-014-05-04 D /IAM/a01/a01-014/a01-014-05-04\n",
      "/words/a01/a01-014/a01-014-06-03 D /IAM/a01/a01-014/a01-014-06-03\n",
      "/words/a01/a01-014/a01-014-06-04 D /IAM/a01/a01-014/a01-014-06-04\n",
      "/words/a01/a01-014/a01-014-07-02 D /IAM/a01/a01-014/a01-014-07-02\n",
      "/words/a01/a01-014/a01-014-07-06 D /IAM/a01/a01-014/a01-014-07-06\n",
      "/words/a01/a01-014/a01-014-08-02 D /IAM/a01/a01-014/a01-014-08-02\n",
      "/words/a01/a01-014/a01-014-09-04 D /IAM/a01/a01-014/a01-014-09-04\n"
     ]
    }
   ],
   "source": [
    "for i in range (100):\n",
    "    print (images[i].inPath,letter,images[i].savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment (path,savePath):\n",
    "    # Converting the input sample image to an array\n",
    "    # Reshaping the input image\n",
    "    datagen = ImageDataGenerator(zoom_range = [1,2],rotation_range = 30,width_shift_range=0.1,height_shift_range=0.1)\n",
    "   \n",
    "    # Loading a sample image \n",
    "    img = load_img(path) \n",
    "    # Converting the input sample image to an array\n",
    "    x = img_to_array(img)\n",
    "    # Reshaping the input image\n",
    "    x = x.reshape((1, ) + x.shape) \n",
    "    \n",
    "    # Generating and saving 5 augmented samples \n",
    "    # using the above defined parameters. \n",
    "    i = 0\n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "    else:\n",
    "        if len(os.listdir(savePath)) > 0:\n",
    "            return\n",
    "    for batch in datagen.flow(x, batch_size = 1,\n",
    "                            save_to_dir =savePath, \n",
    "                            save_prefix ='image', save_format ='jpeg'):\n",
    "        i += 1\n",
    "        if i >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1944207/1944207 [17:51<00:00, 1814.58it/s]\n"
     ]
    }
   ],
   "source": [
    "#execute augment function, will take long time\n",
    "#change\n",
    "last = \"\"\n",
    "total = len(images)\n",
    "last = 0\n",
    "imagesCopy = images.copy()\n",
    "count = len(open(\"progress.txt\",\"r\").read())\n",
    "print(count)\n",
    "progress = open(\"progress.txt\",\"a\")\n",
    "for i in tqdm (range(count,len(images))):\n",
    "    img_ = images[i]\n",
    "    count +=1\n",
    "    augment(dataPath+img_.inPath+\".png\",dataPath+\"/augmentedWords\"+img_.savePath)\n",
    "    progress.write(\".\")\n",
    "progress.close()\n",
    "open(\"progress.txt\",\"w\").close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960591\n"
     ]
    }
   ],
   "source": [
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:54, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7240540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "/home/b1az3/code/ml/trainingData/augmentedWords/EMNIST/44/hsf_6/hsf_6_00130_04448\r"
     ]
    }
   ],
   "source": [
    "\n",
    "count = len(open(\"progress2.txt\",\"r\").read())+1\n",
    "\n",
    "print (count)\n",
    "progress = open(\"progress2.txt\",\"a\")\n",
    "\n",
    "def resize(image_path,size,savePath): #resize to size and add black padding around unknown\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, 1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_with_pad(image, size[0],size[1])\n",
    "    tf.keras.utils.save_img(savePath, image, file_format=\"jpeg\")\n",
    "    os.remove(image_path)\n",
    "total = len(images)\n",
    "last = int(count/total*1000)\n",
    "progressBar = \"\"\n",
    "\n",
    "progressBar = tqdm(range(count,len(images)))\n",
    "print(progressBar)\n",
    "def recurSearch (dir,savedir):\n",
    "    files = os.listdir(dir)\n",
    "    if files[0][-4:] == \"jpeg\":\n",
    "        print (dir,end=\"\\r\")\n",
    "        for file in files:\n",
    "            if not os.path.exists(savedir):\n",
    "                os.makedirs(savedir)\n",
    "            if len(os.listdir(savedir)) < 5:\n",
    "                resize(dir+\"/\"+file,(224,224),savedir+\"/\"+file[:-5])\n",
    "            progress.write(\".\")\n",
    "        progressBar\n",
    "    else:\n",
    "        for file in files:\n",
    "            recurSearch(dir+\"/\"+file,savedir+\"/\"+file)\n",
    "    os.rmdir(dir)\n",
    "\n",
    "\n",
    "recurSearch(dataPath+\"/augmentedWords\",dataPath+\"/final_aug_words\")\n",
    "progress.close()\n",
    "open(\"progress2.txt\",\"w\").close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(dataPath+\"/augmentedWords\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
